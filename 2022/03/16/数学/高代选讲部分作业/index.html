

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Learning_rate">
  <meta name="keywords" content="">
  
    <meta name="description" content="杨 Sir 高代选讲部分作业解答整理，仅供参考">
<meta property="og:type" content="article">
<meta property="og:title" content="Advanced Algebra part of Homework">
<meta property="og:url" content="https://lr-tsinghua11.github.io/2022/03/16/%E6%95%B0%E5%AD%A6/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E9%83%A8%E5%88%86%E4%BD%9C%E4%B8%9A/index.html">
<meta property="og:site_name" content="Learning_rate">
<meta property="og:description" content="杨 Sir 高代选讲部分作业解答整理，仅供参考">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lr-tsinghua11.github.io/img/%E8%93%9D%E5%A4%A9%E7%99%BD%E4%BA%91.jpg">
<meta property="article:published_time" content="2022-03-16T15:09:41.000Z">
<meta property="article:modified_time" content="2023-07-01T06:53:07.024Z">
<meta property="article:author" content="Learning_rate">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lr-tsinghua11.github.io/img/%E8%93%9D%E5%A4%A9%E7%99%BD%E4%BA%91.jpg">
  
  
  <title>Advanced Algebra part of Homework - Learning_rate</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"lr-tsinghua11.github.io","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":80,"cursorChar":".","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"2d5b78dfbf046ab610d306e42da0ed37","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>

  
<meta name="generator" content="Hexo 5.4.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong> Learning_rate&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/%E8%93%9D%E5%A4%A9%E7%99%BD%E4%BA%91.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          <span id="subtitle" title="">
            
          </span>
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-03-16 23:09" pubdate>
          2022年3月16日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          43k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          355 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Advanced Algebra part of Homework</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2023年7月1日 下午
                  
                
              </p>
            
            <div class="markdown-body">
              
              <h1
id="largetextcolorbluemboxadvanced-algebra-small-mathbbhwmathrm2-_textcolorblue2022.3.16"><span
class="math inline">\(\large\textcolor{blue}{\mbox{Advanced Algebra }
\small \mathbb{HW}\mathrm{2}}\ \ \ \ \ \
_\textcolor{blue}{2022.3.16}\)</span></h1>
<h2 id="small-mboxfour-subspaces">1.2.1<span class="math inline">\(\
\small \mbox{four subspaces}\)</span></h2>
<p>Prove or find counter examples.</p>
<ol type="1">
<li>For four subspaces, if any three of them are linearly independent,
then the four subspaces are linearly independent.</li>
<li>If subspaces <span class="math inline">\(V_{1}, V_{2}\)</span> are
linearly independent, and <span class="math inline">\(V_{1}, V_{3},
V_{4}\)</span> are linearly independent, and <span
class="math inline">\(V_{2}, V_{3}, V_{4}\)</span> are linearly
independent, then all four subspaces are linearly independent.</li>
<li>If <span class="math inline">\(V_{1}, V_{2}\)</span> are linearly
independent, and <span class="math inline">\(V_{3}, V_{4}\)</span> are
linearly independent, and <span class="math inline">\(V_{1}+V_{2},
V_{3}+V_{4}\)</span> are linearly independent, then all four subspaces
are linearly independent.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> Construct four subspaces
below. It is obvious that any three of them are linearly independent,
but four subspaces together are linearly dependent. （<span
class="math inline">\(\dim \mathbb {R}^{3}=3\)</span>） <span
class="math display">\[
V_1=\{\begin{bmatrix}k\\0\\0\end{bmatrix}\mid k\in
\mathbb{R}\},V_2=\{\begin{bmatrix}0\\k\\0\end{bmatrix}\mid k\in
\mathbb{R}\},V_3=\{\begin{bmatrix}0\\0\\k\end{bmatrix}\mid k\in
\mathbb{R}\},V_4=\{\begin{bmatrix}k\\k\\k\end{bmatrix}\mid k\in
\mathbb{R}\}
\]</span> <span class="math inline">\((2)\)</span> Construct four
subspaces below. We can prove that each of <span
class="math inline">\(V_1,V_2\)</span> and <span
class="math inline">\(V_1,V_3,V_4\)</span> and <span
class="math inline">\(V_2,V_3,V_4\)</span> are linearly independent.
<span class="math display">\[
V_1=\{\begin{bmatrix}k\\0\\0\end{bmatrix}\mid k\in
\mathbb{R}\},V_2=\{\begin{bmatrix}0\\k\\0\end{bmatrix}\mid k\in
\mathbb{R}\},V_3=\{\begin{bmatrix}k\\0\\k\end{bmatrix}\mid k\in
\mathbb{R}\},V_4=\{\begin{bmatrix}0\\k\\k\end{bmatrix}\mid k\in
\mathbb{R}\}
\]</span> However, pick some special vectors from these subspaces and
its linear combination are zero <span class="math display">\[
1\cdot \begin{bmatrix}1\\0\\0\end{bmatrix}+(-1)\cdot
\begin{bmatrix}0\\1\\0\end{bmatrix}+(-1)\begin{bmatrix}1\\0\\1\end{bmatrix}+1\cdot
\begin{bmatrix}0\\1\\1\end{bmatrix}=\vec{0}
\]</span> <span class="math inline">\((3)\)</span> Reduction to
absurdity, assume that four real numbers that not all of them is zero
<span class="math display">\[
a_1\vec{v}_1+a_2\vec{v}_2+a_3\vec{v}_3+a_4\vec{v}_4=0
\]</span> We can proof that <span
class="math inline">\(a_1\vec{v}_1+a_2\vec{v}_2\neq 0\)</span>,
otherwise according to the independence of <span
class="math inline">\(V_1,V_2\)</span> and</p>
<p><span class="math inline">\(V_3,V_4\)</span> <span
class="math inline">\(a_1=a_2=0,a_3=a_4=0\)</span>, so <span
class="math inline">\(a_1\vec{v}_1+a_2\vec{v}_2\neq
0,a_3\vec{v}_3+a_4\vec{v}_4\neq 0\)</span></p>
<p>But <span class="math inline">\(a_1\vec{v}_1+a_2\vec{v}_2\in
V_1+V_2,a_3\vec{v}_3+a_4\vec{v}_4\in V_3+V_4\)</span>, so linear
combination of <span
class="math inline">\(a_1\vec{v}_1+a_2\vec{v}_2\)</span> and</p>
<p><span class="math inline">\(a_3\vec{v}_3+a_4\vec{v}_4\)</span> can
add up to <span class="math inline">\(0\)</span>, which is contrary to
the independence of <span
class="math inline">\(V_1+V_2.V_3+V_4\)</span></p>
<p>All in all, the four subspaces must be linearly independence</p>
<h2 id="small-mboxdecomposition-of-transpose">1.2.2<span
class="math inline">\(\ \small \mbox{decomposition of
transpose}\)</span></h2>
<p>Let <span class="math inline">\(V\)</span> be the space of <span
class="math inline">\(n \times n\)</span> real matrices. Let <span
class="math inline">\(T: V \rightarrow V\)</span> be the transpose
operation, i.e., <span class="math inline">\(T\)</span> sends <span
class="math inline">\(A\)</span> to <span
class="math inline">\(A^{\mathrm{T}}\)</span> for each <span
class="math inline">\(A \in V\)</span>. Find a non-trivial <span
class="math inline">\(T\)</span>-invariant decomposition of <span
class="math inline">\(V\)</span>, and find the corresponding block form
of <span class="math inline">\(T\)</span>. (Here we use real matrices
for your convenience, but the statement is totally fine for complex
matrices and conjugate transpose.)</p>
<hr />
<p>Set <span class="math inline">\(S=\{A\mid A=A^{T},A\in M_{n\times
n}\}\)</span>, this decomposition is invariant. Because after
transposing any symmetric matrix, the matrix remains itself.</p>
<p>Set <span class="math inline">\(S&#39;=\{A\mid A=-A^{T},A\in
M_{n\times n}\}\)</span>, <span
class="math inline">\(A=-A^{T}\Longrightarrow
A^{T}=-A=-(A^{T})^{T}\)</span>, so any antisymmetric matrix's transpose
is antisymmetric.</p>
<p>So decompose the linear map of <span class="math inline">\(T\)</span>
into <span class="math inline">\(S\)</span> and <span
class="math inline">\(S&#39;\)</span>, <span class="math inline">\(\dim
S=\dfrac{n(n+1)}{2},\dim S&#39;=\dfrac{n(n-1)}{2}\)</span></p>
<p>Because any <span class="math inline">\(n\times n\)</span> matrix
<span
class="math inline">\(B=\dfrac{B+B^{T}}{2}+\dfrac{B-B^{T}}{2}\)</span> ,
so transpose <span class="math inline">\(T\)</span> can be
decomposed</p>
<p>into <span class="math inline">\(S\)</span> and <span
class="math inline">\(S&#39;\)</span> two block form.</p>
<p><span class="math inline">\(A\to
\begin{pmatrix}\frac{A+A^{T}}{2}&amp;0\\0&amp;\frac{A-A^{T}}{2}\end{pmatrix}\)</span>
so the corresponding block form of <span
class="math inline">\(T\)</span> is <span
class="math inline">\(\begin{pmatrix}I&amp;0\\0&amp;-I\end{pmatrix}\)</span></p>
<h2 id="small-mboxultimate-subspaces">1.2.3<span class="math inline">\(\
\small \mbox{ultimate subspaces }\)</span></h2>
<p>Let <span class="math inline">\(p(x)\)</span> be any polynomial, and
define <span class="math inline">\(p(A)\)</span> in the obvious manner.
E.g., if <span class="math inline">\(p(x)=\)</span> <span
class="math inline">\(x^{2}+2 x+3\)</span>, then <span
class="math inline">\(p(A)=A^{2}+2 A+3 I\)</span>. We fix some <span
class="math inline">\(n \times n\)</span> matrix <span
class="math inline">\(A\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(A B=B A\)</span>, show that <span
class="math inline">\(\operatorname{Ker}(B),
\operatorname{Ran}(B)\)</span> are both <span
class="math inline">\(A\)</span>-invariant subspaces.</li>
<li>Prove that <span class="math inline">\(A p(A)=p(A) A\)</span>.</li>
<li>Conclude that <span class="math inline">\(N_{\infty}(A-\lambda I),
R_{\infty}(A-\lambda I)\)</span> are both <span
class="math inline">\(A\)</span>-invariant for any <span
class="math inline">\(\lambda \in \mathbb{C}\)</span>.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> For any vector <span
class="math inline">\(\vec{v}\in
\mbox{Ker}(B),B\vec{v}=\vec{0}\)</span>, so <span
class="math inline">\(B(A\vec{v})=BA\vec{v}=AB\vec{v}=0,A\vec{v}\in
\mbox{Ker}(B)\)</span></p>
<p>And for any vector <span class="math inline">\(\vec{v}\in
\mbox{Ran}(B),B\vec{x}=\vec{v}\)</span>, so <span
class="math inline">\(A\vec{v}=A(B\vec{v})=AB\vec{v}=BA\vec{v}=B(A\vec{v})\)</span></p>
<p><span class="math inline">\(A\vec{v}\in \mbox{Ran}(B)\)</span>. So
<span class="math inline">\(\mbox{Ker}(B)\)</span> and <span
class="math inline">\(\mbox{Ran}(B)\)</span> are both <span
class="math inline">\(A-\)</span>invariant subspaces</p>
<p><span class="math inline">\((2)\)</span> Similar to polynomial, set
<span class="math inline">\(p(A)=\displaystyle
\sum_{i=0}^{n}a_{i}A^{n}\)</span>, so calculate <span
class="math inline">\(Ap(A)\)</span><br />
<span class="math display">\[
Ap(A)=A\displaystyle
\sum_{i=0}^{n}a_iA^{n}=\sum_{i=0}^{n}a_{i}A^{n+1}=(\sum_{i=0}^{n}a_{i}A^{n})A=p(A)A
\]</span> <span class="math inline">\((3)\)</span> Due to limited
dimension <span class="math inline">\(n\)</span> of <span
class="math inline">\(A\)</span>, <span
class="math inline">\(N_{\infty}(A-\lambda I)\)</span> and <span
class="math inline">\(R_{\infty}(A-\lambda I)\)</span> are limited
combinations</p>
<p>of <span class="math inline">\(\mbox{Ker}(A-\lambda I)^{k}\)</span>
and <span class="math inline">\(\mbox{Ran}(A-\lambda I)^{k}\)</span> .
According to the conclusion from <span class="math inline">\(\small
(1)\)</span> and $(2) $</p>
<p><span class="math inline">\(A(A-\lambda I)=(A-\lambda I)A,A(A-\lambda
I)^2=(A-\lambda I)^2A,\cdots ,A(A-\lambda I)^k=(A-\lambda
I)^kA\)</span></p>
<p>so <span class="math inline">\(\forall \ k\in Z\)</span> , <span
class="math inline">\(\mbox{Ker}(A-\lambda I)^{k}\)</span> and <span
class="math inline">\(\mbox{Ran}(A-\lambda I)^{k}\)</span> are all <span
class="math inline">\(A-\)</span>invariant. Add them all together</p>
<p>So, <span class="math inline">\(N_{\infty}(A-\lambda I),
R_{\infty}(A-\lambda I)\)</span> are both <span
class="math inline">\(A\)</span>-invariant for any <span
class="math inline">\(\lambda \in \mathbb{C}\)</span>.</p>
<h2 id="small-mboxinterchangeability-and-common-eigenvector">1.2.4<span
class="math inline">\(\ \small \mbox{interchangeability and common
eigenvector}\)</span></h2>
<p>Note that any linear map must have at least one eigenvector. (You may
try to prove this yourself, but it is not part of this homework.) You
may use this fact freely in this problem. Fix any two <span
class="math inline">\(n \times n\)</span> square matrices <span
class="math inline">\(A, B\)</span>. Suppose <span
class="math inline">\(A B=B A\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(W\)</span> is an A-invariant
subspace, show that <span class="math inline">\(A\)</span> has an
eigenvector in <span class="math inline">\(W\)</span>.</li>
<li>Show that <span class="math inline">\(\operatorname{Ker}(A-\lambda
I)\)</span> is always <span class="math inline">\(B\)</span>-invariant
for all <span class="math inline">\(\lambda \in \mathbb{C}\)</span>.
(Hint: Last problem.)</li>
<li>Show that <span class="math inline">\(A, B\)</span> has a common
eigenvector. (Hint: Last two sub-problem.)</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> Construct a new linear map
from <span class="math inline">\(W\)</span> to <span
class="math inline">\(W\)</span> <span class="math display">\[
W\longmapsto  W: \vec{w}\longmapsto A\vec{w}
\]</span> According to the fact that any linear map must have at least
one eigenvector, A has an eigenvector in <span
class="math inline">\(W\)</span></p>
<p><span class="math inline">\((2)\)</span> For any vector <span
class="math inline">\(\vec{v}\)</span> from <span
class="math inline">\(\mbox{Ker}(A-\lambda I)\)</span>, <span
class="math inline">\((A-\lambda
I)\vec{v}=0,A\vec{v}=\lambda\vec{v}\)</span>, use <span
class="math inline">\(AB=BA\)</span></p>
<p>Because <span class="math inline">\((A-\lambda
I)(B\vec{v})=(AB-\lambda B)\vec{v}=(BA-\lambda B)\vec{v}=B(A-\lambda
I)\vec{v}=0\)</span></p>
<p>So <span class="math inline">\(\mbox{Ker}(A-\lambda I)\)</span> is
<span class="math inline">\(B-\)</span>invariant for all <span
class="math inline">\(\lambda \in \mathbb{C}\)</span></p>
<p><span class="math inline">\((3)\)</span> According to <span
class="math inline">\(\small (1)\)</span> and <span
class="math inline">\(\small (2)\)</span>, there exists at least one
eigenvector <span class="math inline">\(\vec{v}\)</span> in <span
class="math inline">\(\mbox{Ker}(A-\lambda I)\)</span> that</p>
<p><span class="math inline">\(B\vec{v}=\lambda_{B}\vec{v}\)</span> And
the vector <span class="math inline">\(\vec{v}\)</span> also satisfies
that <span
class="math inline">\(A\vec{v}=\lambda_{A}\vec{v}\)</span></p>
<p>So if <span class="math inline">\(A\)</span> and <span
class="math inline">\(B\)</span> are interchangeable, they must have
common eigenvector.</p>
<h1
id="largetextcolorbluemboxadvanced-algebra-small-mathbbhwmathrm3-_textcolorblue2022.3.24"><span
class="math inline">\(\large\textcolor{blue}{\mbox{Advanced Algebra }
\small \mathbb{HW}\mathrm{3}}\ \ \ \ \ \
_\textcolor{blue}{2022.3.24}\)</span></h1>
<h2 id="small-mboxjordan-normal-form">1.3.1<span class="math inline">\(\
\small \mbox{jordan normal form}\)</span></h2>
<p>Find a basis in the following vector space so that the linear map
involved will be in Jordan normal form. Also find the Jordan normal
form.</p>
<ol type="1">
<li><span class="math inline">\(V=\mathbb{C}^{2}\)</span> is a real
vector space, and <span class="math inline">\(A: V \rightarrow
V\)</span> that sends <span
class="math inline">\(\left[\begin{array}{l}x \\
y\end{array}\right]\)</span> to <span
class="math inline">\(\left[\begin{array}{c}\bar{x}-\Re(y) \\ (1+i)
\Im(x)-y\end{array}\right]\)</span> is a real linear map. (Here <span
class="math inline">\(\bar{x}\)</span> means the complex conjugate of a
complex number <span class="math inline">\(x\)</span>, and <span
class="math inline">\(\Re(x), \Im(x)\)</span> means the real part and
the imaginary part of a complex number <span class="math inline">\(x
.)\)</span></li>
<li><span class="math inline">\(V=P_{4}\)</span>, the real vector space
space of all real polynomials of degree at most 4. And <span
class="math inline">\(A: V \rightarrow V\)</span> is a linear map such
that <span
class="math inline">\(A(p(x))=p^{\prime}(x)+p(0)+p^{\prime}(0)
x^{2}\)</span> for each polynomial <span class="math inline">\(p \in
P_{4}\)</span>.</li>
<li><span class="math inline">\(A=\left[\begin{array}{llll} &amp; &amp;
&amp; a_{1} \\ &amp; &amp; a_{2} &amp; \\ &amp; a_{3} &amp; &amp; \\
a_{4} &amp; &amp; &amp; \end{array}\right]\)</span>. Be careful here.
Maybe we have many possibilities for its Jordan normal form depending on
the values of <span class="math inline">\(a_{1}, a_{2}, a_{3},
a_{4}\)</span>.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> <span
class="math inline">\(A_1=\begin{pmatrix}1&amp;0&amp;-1&amp;0\\0&amp;-1&amp;0&amp;0\\0&amp;1&amp;-1&amp;0\\0&amp;1&amp;0&amp;-1\end{pmatrix}=\begin{pmatrix}1&amp;2&amp;1&amp;0\\0&amp;0&amp;4&amp;0\\0&amp;4&amp;0&amp;0\\0&amp;4&amp;0&amp;1\end{pmatrix}\begin{pmatrix}1&amp;0&amp;0&amp;0\\0&amp;-1&amp;1&amp;0\\0&amp;0&amp;-1&amp;0\\0&amp;0&amp;0&amp;-1\end{pmatrix}\begin{pmatrix}1&amp;2&amp;1&amp;0\\0&amp;0&amp;4&amp;0\\0&amp;4&amp;0&amp;0\\0&amp;4&amp;0&amp;1\end{pmatrix}^{-1}\)</span></p>
<p><span class="math inline">\((2)\)</span> <span
class="math inline">\(A:a_0+a_1x+a_2x^2+a_3x^3+a_4x^4\longmapsto(a_0+a_1)+2a_2x+(a_1+2a_3)x^2+4a_4x^3\)</span>
<span class="math display">\[
\begin{gathered}
\begin{pmatrix}1&amp;1&amp;0&amp;0&amp;0\\0&amp;0&amp;2&amp;0&amp;0\\0&amp;1&amp;0&amp;3&amp;0\\0&amp;0&amp;0&amp;0&amp;4\\0&amp;0&amp;0&amp;0&amp;0\end{pmatrix}=J\begin{pmatrix}1&amp;0&amp;0&amp;0&amp;0\\0&amp;\sqrt{2}&amp;0&amp;0&amp;0\\0&amp;0&amp;-\sqrt{2}&amp;0&amp;0\\0&amp;0&amp;0&amp;0&amp;1\\0&amp;0&amp;0&amp;0&amp;0\end{pmatrix}J^{-1}\\
J=\begin{pmatrix}1&amp;\sqrt{2}+1&amp;1-\sqrt{2}&amp;12&amp;12\\0&amp;1&amp;1&amp;-12&amp;0\\0&amp;\dfrac{\sqrt{2}}{2}&amp;-\dfrac{\sqrt{2}}{2}&amp;0&amp;-6\\0&amp;0&amp;0&amp;4&amp;0\\0&amp;0&amp;0&amp;0&amp;1\end{pmatrix}
\end{gathered}
\]</span> <span class="math inline">\((3)\)</span> <span
class="math display">\[
J=\begin{pmatrix}J_{1,4}&amp;O\\O&amp;J_{2,3}\end{pmatrix},\mbox{where
}J_{i,j}=\begin{cases}\begin{pmatrix}\sqrt{a_ia_j}&amp;0\\0&amp;-\sqrt{a_ia_j}\end{pmatrix}&amp;a_i\neq
0,a_j\neq 0\\\begin{pmatrix}0&amp;0\\0&amp;0\end{pmatrix}&amp;a_i=
0,a_j\neq 0\mbox{ or }a_i\neq 0,a_j=0
\\\begin{pmatrix}0&amp;1\\0&amp;0\end{pmatrix}&amp;a_i=a_j=0\\\end{cases}
\]</span></p>
<h2 id="small-mboxpartitions-of-interger">1.3.2<span
class="math inline">\(\ \small \mbox{partitions of
interger}\)</span></h2>
<p>A partition of integer <span class="math inline">\(n\)</span> is a
way to write <span class="math inline">\(n\)</span> as a sum of other
positive integers, say <span class="math inline">\(5=2+2+1\)</span>. If
you always order the summands from large to small, you end up with a dot
diagram, where each column represent an integer: <span
class="math inline">\(\left[\begin{array}{ll}\cdot &amp; \cdot \\ \cdot
&amp; \cdot \\ \cdot\end{array}\right]\)</span>. Similarly, <span
class="math inline">\(7=4+2+1\)</span> should be represented as <span
class="math inline">\(\left[\begin{array}{lll} \cdot &amp; \cdot &amp;
\cdot \\ \cdot &amp; \cdot &amp; \\ \cdot &amp; &amp; \\ \cdot &amp;
&amp; \end{array}\right]\)</span></p>
<ol type="1">
<li><p>If the Jordan normal form of an <span class="math inline">\(n
\times n\)</span> nilpotent matrix <span
class="math inline">\(A\)</span> is diag <span
class="math inline">\(\left(J_{a_{1}}, J_{a_{2}}, \ldots,
J_{a_{k}}\right)\)</span>, then we have a partition of integer <span
class="math inline">\(n=a_{1}+\ldots+a_{k}\)</span>. However, we also
have a partition of integer <span class="math inline">\(n=\small
[\operatorname{dim} \operatorname{Ker}(A)]+\left[\operatorname{dim}
\operatorname{Ker}\left(A^{2}\right)-\operatorname{dim}
\operatorname{Ker}(A)\right]+\left[\operatorname{dim}
\operatorname{Ker}\left(A^{3}\right)-\operatorname{dim}
\operatorname{Ker}\left(A^{2}\right)\right]+\ldots\)</span> where we
treat the content of each bracket as a positive integer. Can you find a
relation between the two dot diagrams?</p></li>
<li><p>A partition of integer <span
class="math inline">\(n=a_{1}+\ldots+a_{k}\)</span> is called
self-conjugate if, for the matrix <span
class="math inline">\(A=\operatorname{diag}\left(J_{a_{1}}, J_{a_{2}},
\ldots, J_{a_{k}}\right)\)</span>, the two dot diagrams you obtained
above are the same. Show that, for a fixed integer n, the number of
self-conjugate partition of <span class="math inline">\(n\)</span> is
equal to the number of partition of <span
class="math inline">\(n\)</span> into distinct odd positive integers.
(Hint: For a self-conjugate dot diagram, count the total number of dots
that are either in the first column or in the first row or in both. Is
this always odd?)</p></li>
<li><p>Suppose a 4 by 4 matrix <span class="math inline">\(A\)</span> is
nilpotent and upper trianguler, and all <span class="math inline">\((i,
j)\)</span> entries for <span class="math inline">\(i&lt;j\)</span> are
chosen randomly and uniformly in the interval <span
class="math inline">\([-1,1]\)</span>. What are the probabilities that
its Jordan canonical form corresponds to the partitions <span
class="math inline">\(4=4,4=3+1,4=2+2,4=2+1+1,4=1+1+1+1\)</span>
?</p></li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> I can find that the sequence
of each bracket' number is not incremental, which is just like the dot
graph. Put <span class="math inline">\(\small [\operatorname{dim}
\operatorname{Ker}(A)],\left[\operatorname{dim}
\operatorname{Ker}\left(A^{2}\right)-\operatorname{dim}
\operatorname{Ker}(A)\right],\left[\operatorname{dim}
\operatorname{Ker}\left(A^{3}\right)-\operatorname{dim}
\operatorname{Ker}\left(A^{2}\right)\right],\ldots\)</span> like the dot
graph. According to the 'killing chain', the number of each row's dots
of the dot graph is just <span
class="math inline">\(a_1,a_2,a_3,\ldots\)</span></p>
<p><span class="math inline">\((2)\)</span> The self-conjugate of the
partition is just like a flying wing. Like this <span
class="math inline">\(\left[\begin{array}{lll} \cdot &amp; \cdot &amp;
\cdot &amp; \cdot&amp;\cdot &amp;\cdot \\ \cdot &amp; \cdot &amp; \cdot
&amp; \cdot&amp;\cdot \\ \cdot &amp;\cdot &amp;\cdot &amp; \\\cdot
&amp;\cdot &amp;\\ \cdot &amp;\cdot &amp;\\\cdot
\end{array}\right]\)</span></p>
<p>If rudely call this as a matrix <span
class="math inline">\(A\)</span>, because of the condition, we have
<span class="math inline">\(A=A^{T}\)</span>.</p>
<p>Consider the outermost corner, the total number of the dots is <span
class="math inline">\(n+n-1=2n-1\in \mbox{odd}\)</span></p>
<p>And deprive each corner, the newest outermost corner also satisfies
the odd condition.</p>
<p>So every self-conjugate can be correspondence to odd-partition. And
for each odd-partition, we can construct the matrix one corner by one
corner.</p>
<p>In conclusion, the two numbers are the same.</p>
<p><span class="math inline">\((3)\)</span> <span
class="math inline">\(A\)</span> is nilpotent and its dimension is <span
class="math inline">\(4\)</span>, so <span
class="math inline">\(A^{4}=O\)</span>. Consider the diagonal
elements.</p>
<p><span class="math inline">\(A^4(i,i)=(A(i,i))^4=0\)</span>, so <span
class="math inline">\(A\)</span> is like <span
class="math inline">\(A=\begin{pmatrix}0&amp;\mbox{ran}&amp;\mbox{ran}&amp;\mbox{ran}\\0&amp;0&amp;\mbox{ran}&amp;\mbox{ran}\\0&amp;0&amp;0&amp;\mbox{ran}\\0&amp;0&amp;0&amp;0\end{pmatrix}\)</span>.
It is obvious that all the</p>
<p>eigenvalues are <span class="math inline">\(0\)</span>. Consider
<span
class="math inline">\(\mbox{Ker}(A),\mbox{Ker}(A^2),\mbox{Ker}(A^3),\mbox{Ke}(A^4)\)</span>,
dimension of each is nearly</p>
<p><span class="math inline">\(1,2,3,4\)</span> , since each <span
class="math inline">\(\mbox{ran}\)</span> is randomly chosen from <span
class="math inline">\([-1,1]\)</span>. The probability <span
class="math inline">\(4=1+1+1+1\)</span></p>
<p>is <span class="math inline">\(1\)</span>, others are <span
class="math inline">\(0\)</span>.</p>
<h1
id="largetextcolorbluemboxadvanced-algebra-small-mathbbhwmathrm6-_textcolorblue2022.4.29"><span
class="math inline">\(\large\textcolor{blue}{\mbox{Advanced Algebra }
\small \mathbb{HW}\mathrm{6}}\ \ \ \ \ \
_\textcolor{blue}{2022.4.29}\)</span></h1>
<h2 id="small-mboxvandermode-matrix">1.6.1<span class="math inline">\(\
\small \mbox{Vandermode matrix}\)</span></h2>
<p>Let <span class="math inline">\(V\)</span> be the space of real
polynomials of degree less than <span class="math inline">\(n\)</span>.
So <span class="math inline">\(\operatorname{dim} V=n\)</span>. Then for
each <span class="math inline">\(a \in \mathbb{R}\)</span>, the
evaluation <span class="math inline">\(\mathrm{ev}_{a}\)</span> is a
dual vector.</p>
<p>For any real numbers <span class="math inline">\(a_{1}, \ldots, a_{n}
\in \mathbb{R}\)</span>, consider the map <span class="math inline">\(L:
V \rightarrow \mathbb{R}^{n}\)</span> such that <span
class="math inline">\(L(p)=\left[\begin{array}{c}p\left(a_{1}\right) \\
\vdots \\ p\left(a_{n}\right)\end{array}\right]\)</span>.</p>
<ol type="1">
<li>Write out the matrix for <span class="math inline">\(L\)</span>
under the basis <span class="math inline">\(1, x, \ldots,
x^{n-1}\)</span> for <span class="math inline">\(V\)</span> and the
standard basis for <span class="math inline">\(\mathbb{R}^{n}\)</span>.
(Do you know the name for this matrix?)</li>
<li>Prove that <span class="math inline">\(L\)</span> is invertible if
and only if <span class="math inline">\(a_{1}, \ldots, a_{n}\)</span>
are distinct. (If you can name the matrix <span
class="math inline">\(L\)</span>, then you may use its determinant
formula without proof.)</li>
<li>Show that <span class="math inline">\(\mathrm{ev}_{a_{1}}, \ldots,
\mathrm{ev}_{a_{n}}\)</span> form a basis for <span
class="math inline">\(V^{*}\)</span> if and only if all <span
class="math inline">\(a_{1}, \ldots, a_{n}\)</span> are distinct.</li>
<li>Set <span class="math inline">\(n=3\)</span>. Find polynomials <span
class="math inline">\(p_{1}, p_{2}, p_{3}\)</span> such that <span
class="math inline">\(p_{i}(j)=\delta_{i j}\)</span> for <span
class="math inline">\(i, j \in\{-1,0,1\}\)</span>.</li>
<li>Set <span class="math inline">\(n=4\)</span>, and consider <span
class="math inline">\(\mathrm{ev}_{-2}, \mathrm{ev}_{-1},
\mathrm{ev}_{0}, \mathrm{ev}_{1}, \mathrm{ev}_{2} \in V^{*}\)</span>.
Since <span class="math inline">\(\operatorname{dim} V^{*}=4\)</span>,
these must be linearly dependent. Find a non-trivial linear combination
of these which is zero.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> <span
class="math inline">\(L=\left[\begin{array}{cccc} 1 &amp; a_{1} &amp;
\cdots &amp; a_{1}^{n-1} \\ 1 &amp; a_{2} &amp; \cdots &amp; a_{2}^{n-1}
\\ \vdots &amp; &amp; &amp; \vdots \\ 1 &amp; a_{n} &amp; \cdots &amp;
a_{n}^{n-1} \end{array}\right]\)</span><strong>Vandermode
matrix</strong></p>
<p>Calculate its inverse and we can get standard basis using Lagrange
interpolation. <span class="math display">\[
\left(\begin{array}{cccc}
1 &amp; a_{1} &amp; \cdots &amp; a_{1}^{n-1} \\
1 &amp; a_{2} &amp; \cdots &amp; a_{2}^{n-1} \\
\vdots &amp; &amp; &amp; \vdots \\
1 &amp; a_{n} &amp; \cdots &amp; a_{n}^{n-1}
\end{array}\right)\left(\begin{array}{c}x_{0} \\ x_{1} \\ x_{2} \\
\vdots \\ x_{n-1}\end{array}\right)=\left(\begin{array}{c}y_{0} \\ y_{1}
\\ y_{2} \\ \vdots \\
y_{n-1}\end{array}\right)\\\left(\begin{array}{cccc}
1 &amp; a_{1} &amp; \cdots &amp; a_{1}^{n-1} \\
1 &amp; a_{2} &amp; \cdots &amp; a_{2}^{n-1} \\
\vdots &amp; &amp; &amp; \vdots \\
1 &amp; a_{n} &amp; \cdots &amp; a_{n}^{n-1}
\end{array}\right)^{-1}\left(\begin{array}{c}y_{0} \\ y_{1} \\ y_{2} \\
\vdots \\ y_{n-1}\end{array}\right)=\left(\begin{array}{c}x_{0} \\ x_{1}
\\ x_{2} \\ \vdots \\ x_{n-1}\end{array}\right)
\]</span> Construct polynomial <span
class="math inline">\(f(a)=\displaystyle \sum_{i}y_i \prod_{j \neq i}
\frac{a-a_{j}}{a_{i}-a_{j}}\)</span> So <span
class="math inline">\(f(a_i)=x_i\)</span></p>
<p>So <span class="math inline">\(\left(V^{-1}\right)_{i j}\)</span> is
the coefficient of <span class="math inline">\(\displaystyle \prod_{k
\neq i} \frac{a-a_{k}}{a_{i}-a_{k}}\)</span> at <span
class="math inline">\(x^{j-1}\)</span> , which is <span
class="math inline">\(\displaystyle\left(V^{-1}\right)_{i
j}=\left[x^{j-1}\right] \prod_{k \neq i}
\frac{a-a_{k}}{a_{i}-a_{k}}\)</span> <span class="math display">\[
\left(V^{-1}\right)_{i j}=(-1)^{j+1} \frac{\displaystyle \sum_{0 \leq
p_{1}&lt;\cdots&lt;p_{n-\zeta} ; p_{1}, p_{2}, \cdots p_{n-j} \neq i}
x_{p_{1}} x_{p_{2}} \cdots x_{p_{n-j}}}{\displaystyle \prod_{0 \leq
k&lt;n ; k \neq i}\left(x_{k}-x_{i}\right)}
\]</span> The column of the <span
class="math inline">\((V^{-1})\)</span> is the standard basis of <span
class="math inline">\(V\)</span></p>
<p><span class="math inline">\((2)\)</span> <span
class="math inline">\(\det L=\displaystyle \prod_{1\leq i\leq j\leq n
}(a_i-a_j)\neq 0\Longleftrightarrow a_i\neq a_j\)</span></p>
<p><span class="math inline">\((3)\)</span> <span
class="math inline">\(e v_{a_{1}}, \cdots, ev_{a_n}\)</span> form a
basis for <span class="math inline">\(V^{*} \Longleftrightarrow e
v_{a_{1}}, e v_{a_{2}}, \cdots, ev_{a_n}\)</span>, evan are linearly
indepenctent.</p>
<p>The matrix <span class="math inline">\(\left(\begin{array}{c}e
v_{a_{1}} \\ e v_{a_{2}} \\ \vdots \\ e
v_{a_{n}}\end{array}\right)\)</span> is invertible, which means <span
class="math inline">\(L\)</span> is invertible. According to <span
class="math inline">\(\small(2)\)</span>, <span
class="math inline">\(a_i\)</span> are distinct.</p>
<p><span class="math inline">\((4)\)</span> Pick original basis <span
class="math inline">\(\{1,x,x^2\}\)</span> So <span
class="math inline">\(\alpha_{1}=(1,-1,1) \quad \alpha_{2}=(1,0,0) \quad
\alpha_{3}=(1,1,1)\)</span> <span class="math display">\[
A=\left(\begin{array}{l}\alpha_{1} \\ \alpha_{2} \\
\alpha_{3}\end{array}\right)=\left(\begin{array}{ccc}1 &amp; -1 &amp; 1
\\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1\end{array}\right) \quad
A^{-1}=\left(\begin{array}{ccc}0 &amp; 1 &amp; 0 \\ -\frac{1}{2} &amp; 0
&amp; \frac{1}{2} \\ \frac{1}{2} &amp; -1 &amp;
\frac{1}{2}\end{array}\right)
\]</span> So <span class="math inline">\(p_{1}=-\dfrac{1}{2}
x+\dfrac{1}{2} x^{2} \quad p_{2}=1-x^{2} \quad p_{3}=\dfrac{1}{2}
x+\dfrac{1}{2} x^{2}\)</span></p>
<p><span class="math inline">\((5)\)</span> Set <span
class="math inline">\(p(x)=ax^3+bx^2+cx+d\)</span> and <span
class="math display">\[m e v_{-2}+n e v_{-1}+p e v_{0}+q e v_{1}+r e
v_{2}=0\]</span> <span class="math display">\[
\begin{gathered}
e v_{-2}=-8 a+4 b-2 c+d \quad e v_{-1}=-a+b-c+d \quad e v_{0}=d\\ \quad
e v_{1}=a+b+c+d \quad e v_{2}=8 a+4 b+2 c+d\\
\left(\begin{array}{ccccc}-8 &amp; -1 &amp; 0 &amp; 1 &amp; 8 \\ 4 &amp;
1 &amp; 0 &amp; 1 &amp; 4 \\ -2 &amp; -1 &amp; 0 &amp; 1 &amp; 2 \\ 1
&amp; 1 &amp; 1 &amp; 1 &amp;1\end{array}\right)\left(\begin{array}{l}m
\\ n \\ p \\ q \\ r\end{array}\right)=\boldsymbol 0
\end{gathered}
\]</span> Solve that <span class="math inline">\(e v_{-2}-4 e v_{-1}+6 e
v_{0}-4 e v_{1}+e v_{2}=0\)</span></p>
<h2 id="small-mboxdual-vector-in-polynomials">1.6.2<span
class="math inline">\(\ \small \mbox{dual vector in
polynomials}\)</span></h2>
<p>Let <span class="math inline">\(V\)</span> be the space of real
polynomials of degree less than <span class="math inline">\(3\)</span>.
Which of the following is a dual vector? Prove it or show why not.</p>
<ol type="1">
<li><span class="math inline">\(p \mapsto \operatorname{ev}_{5}((x+1)
p(x))\)</span>.</li>
<li><span class="math inline">\(p \mapsto \lim _{x \rightarrow \infty}
\dfrac{p(x)}{x}\)</span>.</li>
<li><span class="math inline">\(p \mapsto \lim _{x \rightarrow \infty}
\dfrac{p(x)}{x^{2}}\)</span>.</li>
<li><span class="math inline">\(p \mapsto p(3)
p^{\prime}(4)\)</span>.</li>
<li><span class="math inline">\(p \mapsto
\operatorname{deg}(p)\)</span>, the degree of the polynomial <span
class="math inline">\(p\)</span>.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> Yes. <span
class="math inline">\(①\  e_{s}((x+1) p(x))=6 p(5)\)</span>. that is the
map from <span class="math inline">\(V\)</span> to <span
class="math inline">\(\mathbb{R}\)</span>. <span
class="math inline">\(②\)</span> For <span
class="math inline">\(\forall\  p, q \in V\)</span> and</p>
<p><span class="math inline">\(\forall m, n \in \mathbb{R}, L(m p+n q)=6
m p(5)+6 n p(5)=m L(p)+n L(q)\)</span>, so its bilinear.</p>
<p><span class="math inline">\((2)\)</span> No. Sometimes the limit
doeen't exist when <span class="math inline">\(\mbox{deg}(p)\geq
2\)</span></p>
<p><span class="math inline">\((3)\)</span> Yes. This vector is just a
'taking the coefficient of <span class="math inline">\(x^2\)</span>',
which is bilinear.</p>
<p><span class="math inline">\((4)\)</span> No. For instance, <span
class="math inline">\(p(x)=x^{2}+2, q(x)=-2 x, \quad L(p)=11 \times 8=88
\quad L(q)=12\)</span></p>
<p><span class="math inline">\(L(p+q)=5 \times 6=30 . \quad L(p+q) \neq
L(p)+L(q)\)</span>. So it's not a dual vector.</p>
<p><span class="math inline">\((5)\)</span> No. For instance, <span
class="math inline">\(p(x)=x^{2}+x, q(x)=-x^{2}+1,
L(p)=\operatorname{deg}(p)=2, L(q)=\operatorname{deg}(q)=2\)</span></p>
<p><span class="math inline">\(L(p+q)=\operatorname{deg}(x+1)=1 .
L(p)+L(q) \neq L(p+q)\)</span>. So it's not a clual vector</p>
<h2 id="small-mboxdirectional-derivative">1.6.3<span
class="math inline">\(\ \small \mbox{directional
derivative}\)</span></h2>
<p>Fix a differentiable function <span class="math inline">\(f:
\mathbb{R}^{2} \rightarrow \mathbb{R}\)</span>, and fix a point <span
class="math inline">\(\boldsymbol{p} \in \mathbb{R}^{2}\)</span>. For
any vector <span class="math inline">\(\boldsymbol{v} \in
\mathbb{R}^{2}\)</span>, then the directional derivative of <span
class="math inline">\(f\)</span> at <span
class="math inline">\(\boldsymbol{p}\)</span> in the direction of <span
class="math inline">\(\boldsymbol{v}\)</span> is defined as <span
class="math inline">\(\nabla_{\boldsymbol{v}} f:=\lim _{t \rightarrow 0}
\dfrac{f(\boldsymbol{p}+t \boldsymbol{v})-f(\boldsymbol{p})}{t} . S
\mathrm{Show}\)</span> that the map <span class="math inline">\(\nabla
f: \boldsymbol{v} \mapsto \nabla_{\boldsymbol{v}}(f)\)</span> is a dual
vector in <span
class="math inline">\(\left(\mathbb{R}^{2}\right)^{*}\)</span>, i.e., a
row vector. Also, what are its "coordinates" under the standard dual
basis? (Remark: In calculus, we write <span class="math inline">\(\nabla
f\)</span> as a column vector for historical reasons. By all means, from
a mathematical perspective, the correct way to write <span
class="math inline">\(\nabla f\)</span> is to write it as a row vector,
as illlustrated in this problem. (But don't annoy your calculus teachers
though.... In your calculus class, you use whatever notation your
calculus teacher told you.) (Extra Remark: If we use row vector, then
the evaluation of <span class="math inline">\(\nabla f\)</span> at <span
class="math inline">\(\boldsymbol{v}\)</span> is purely linear, and no
inner product structure is needed, which is sweet. But if we HAVE TO
write <span class="math inline">\(\nabla f\)</span> as a column vector
(for historical reason), then we would have to do a dot product between
<span class="math inline">\(\nabla f\)</span> and <span
class="math inline">\(\boldsymbol{v}\)</span>, which now requires an
inner product structure. That is an unnecessary dependence on an extra
structure that actually should have no influence.)</p>
<hr />
<p>Set <span
class="math inline">\(\vec{v}=\begin{pmatrix}a\\b\end{pmatrix}\)</span>,
since the function <span class="math inline">\(f\)</span> is
differentiable, <span class="math inline">\(\nabla_{\vec{v}}
f=\dfrac{\partial f(\vec{v})}{\partial x} \cdot
\dfrac{a}{\sqrt{a^{2}+b^{2}}}+\dfrac{\partial f(\vec{v})}{\partial y}
\dfrac{b}{\sqrt{a^{2}+b}}\)</span></p>
<p>So the map <span class="math inline">\(\nabla f\)</span> is a map
from <span
class="math inline">\(\left(\mathbb{R}^{2}\right)^{*}\)</span> to <span
class="math inline">\(\mathbb{R}\)</span>.And for <span
class="math inline">\(\forall\)</span> diffential functions <span
class="math inline">\(f, g, \forall m, n \in \mathbb{R}\)</span>, we
have: <span class="math display">\[
\begin{aligned}
\nabla_{\vec{v}}(m f+n g) &amp;=\frac{\partial[m f(\vec{v})+n
g(\vec{v})]}{\partial x} \cdot
\frac{a}{\sqrt{a^{2}+b^{2}}}+\frac{\partial[m f(\vec{v})+n
g(\vec{v})]}{\partial y} \cdot \frac{b}{\sqrt{a^{2}+b^{2}}} \\
&amp;=m \nabla_{\vec{v}} f+n \nabla_{\vec{v}} g
\end{aligned}
\]</span> So the map <span class="math inline">\(\nabla f\)</span> is
bilinear, i.e, is a dual vector in <span
class="math inline">\(\left(\mathbb{R}^{2}\right)^{*}\)</span>.</p>
<p>Since <span class="math inline">\(x\)</span>-axis and <span
class="math inline">\(y\)</span>-axis are perpendicular, the standard
dual basis is <span class="math inline">\(\dfrac{\partial f}{\partial
x}\)</span> and <span class="math inline">\(\dfrac{\partial f}{\partial
y} .\)</span> And the coordinates are <span
class="math inline">\(\dfrac{\vec{v} \cdot
\hat{x}}{\|\vec{v}\|}\)</span> and <span
class="math inline">\(\dfrac{\vec{v} \cdot
\hat{y}}{\|\vec{v}\|}\)</span></p>
<h2 id="section">1.6.4$ $</h2>
<p>Consider a linear map <span class="math inline">\(L: V \rightarrow
W\)</span> and its dual map <span class="math inline">\(L^{*}: W^{*}
\rightarrow V^{*}\)</span>. Prove the following.</p>
<ol type="1">
<li><span
class="math inline">\(\operatorname{Ker}\left(L^{*}\right)\)</span> is
exactly the collection of dual vectors in <span
class="math inline">\(W^{*}\)</span> that kills <span
class="math inline">\(\operatorname{Ran}(L)\)</span>.</li>
<li><span
class="math inline">\(\operatorname{Ran}\left(L^{*}\right)\)</span> is
exactly the collection of dual vectors in <span
class="math inline">\(V^{*}\)</span> that kills <span
class="math inline">\(\operatorname{Ker}(L)\)</span>.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> First, all the elements in
<span class="math inline">\(\ker(L^*)\in W^{*}\)</span>, so just prove
<span class="math inline">\(\forall \ \vec{v}\in
\ker(L^*),\vec{\omega}\in \mbox{Ran}(L),\vec{v}^{T}\cdot
\vec{\omega}=0\)</span></p>
<p><span
class="math inline">\(L^{*}\vec{v}=0,L^{*}\vec{v}\cdot\vec{x}=0=\vec{v}^{T}(L\vec{x})\)</span>,
set <span class="math inline">\(L\vec{x}=\vec{\omega}\)</span> So <span
class="math inline">\(\vec{v}^{T}\cdot \vec{\omega}=\langle
\vec{v},\vec{\omega}\rangle=0\)</span></p>
<p><span class="math inline">\((2)\)</span> First, all the elements in
<span class="math inline">\(\mbox{Ran}(L^{*})\in V^{*}\)</span> ,so just
prove <span class="math inline">\(\forall \ \vec{v}\in
\mbox{Ran}(L^*),\vec{\omega}\in \mbox{Ker}(L),\vec{v}^{T}\cdot
\vec{\omega}=0\)</span></p>
<p><span
class="math inline">\(L\vec{\omega}=0,L\vec{\omega}\cdot\vec{x}=0=\vec{\omega}^{T}(L^{*}\vec{x})\)</span>,
set <span class="math inline">\(L^{*}\vec{x}=\vec{v}\)</span>, So <span
class="math inline">\(\vec{\omega}^{T}\cdot \vec{v}=\langle
\vec{\omega},\vec{v}\rangle=0\)</span></p>
<h1
id="largetextcolorbluemboxadvanced-algebra-small-mathbbhwmathrm7-_textcolorblue2022.5.3"><span
class="math inline">\(\large\textcolor{blue}{\mbox{Advanced Algebra }
\small \mathbb{HW}\mathrm{7}}\ \ \ \ \ \
_\textcolor{blue}{2022.5.3}\)</span></h1>
<h2 id="small-mboxbra-map-and-riesz-map">1.7.1<span
class="math inline">\(\ \small \mbox{bra map and Riesz
map}\)</span></h2>
<p>On the space <span class="math inline">\(\mathbb{R}^{n}\)</span>, we
fix a symmetric positive-definite matrix <span
class="math inline">\(A\)</span>, and define <span
class="math inline">\((\boldsymbol{v},
\boldsymbol{w})=\boldsymbol{v}^{\mathrm{T}}A\boldsymbol{w}\)</span></p>
<ol type="1">
<li>Show that this is an inner product.</li>
<li>The Riesz map (inverse of the bra map) from <span
class="math inline">\(V^{*}\)</span> to <span
class="math inline">\(V\)</span> would send a row vector <span
class="math inline">\(\boldsymbol{v}^{\mathrm{T}}\)</span> to what?</li>
<li>The bra map from <span class="math inline">\(V\)</span> to <span
class="math inline">\(V^{*}\)</span> would send a vector <span
class="math inline">\(\boldsymbol{v}\)</span> to what?</li>
<li>The dual of the Riesz map from <span
class="math inline">\(V^{*}\)</span> to <span
class="math inline">\(V\)</span> would send a row vector <span
class="math inline">\(\boldsymbol{v}^{\mathrm{T}}\)</span> to what?</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> Easy to prove.</p>
<p><span class="math inline">\((2)\)</span> The bra map means <span
class="math inline">\(\forall \ \langle \boldsymbol{v}|\in
\mathcal{B},s.t.\langle
\boldsymbol{v}|:\boldsymbol{\omega\longmapsto}\langle\boldsymbol{v},\boldsymbol{\omega}\rangle=\boldsymbol{v}^{\mathrm{T}}A\boldsymbol{w}\)</span>
So the inverse of <span class="math inline">\(\langle
\boldsymbol{v}|\)</span> is make <span
class="math inline">\(\boldsymbol{v}^{\mathrm{T}}A\)</span></p>
<p>to <span class="math inline">\(\boldsymbol {v}\)</span>, which is
from <span class="math inline">\(V^{*}\)</span> to <span
class="math inline">\(V\)</span>. Set <span
class="math inline">\(\boldsymbol{u}=\boldsymbol {v}^{T}A\)</span>, so
<span
class="math inline">\(\boldsymbol{v}^{T}=\boldsymbol{u}A^{-1}\)</span>
then transpose it we can get <span
class="math inline">\(\boldsymbol{v}=(A^{-1})^{T}\boldsymbol{u^{T}}\)</span></p>
<p>So if the input is <span class="math inline">\(\boldsymbol{v}^{T}\in
V^{*}\)</span>, the output is <span
class="math inline">\((A^{-1})^{T}(\boldsymbol
v^{T})^{T}=(A^{-1})^{T}\boldsymbol{v}\in V\)</span></p>
<p><span class="math inline">\((3)\)</span> Obviously, it sends <span
class="math inline">\(\boldsymbol{v}\)</span> to <span
class="math inline">\(\boldsymbol{v}^{T}A\)</span></p>
<p><span class="math inline">\((4)\)</span> I guess the result is the
same with <span class="math inline">\(\small(2)\)</span>, which sends
<span class="math inline">\(\boldsymbol {v}^{T}\)</span> to <span
class="math inline">\((A^{-1})^{T}\boldsymbol{v}\)</span></p>
<h2 id="small-mboxwhat-is-a-derivative">1.7.2<span
class="math inline">\(\ \small \mbox{What is a derivative}\)</span></h2>
<p>The discussions in this problem holds for all manifolds <span
class="math inline">\(M\)</span>. But for simplicities sake, suppose
<span class="math inline">\(M=\mathbb{R}^{3}\)</span> for this
problem.</p>
<p>Let <span class="math inline">\(V\)</span> be the space of all
analytic functions from <span class="math inline">\(M\)</span> to <span
class="math inline">\(\mathbb{R}\)</span>. Here analytic means <span
class="math inline">\(f(x, y, z)\)</span> is a infinite polynomial
series (its Taylor expansion) with variables <span
class="math inline">\(x, y, z\)</span>. Approximately <span
class="math inline">\(f(x, y, z)=a_{0}+a_{1} x+a_{2} y+\)</span> <span
class="math inline">\(a_{3} z+a_{4} x^{2}+a_{5} x y+a_{6} x z+a_{7}
y^{2}+\ldots\)</span>, and things should converge always.</p>
<p>Then a dual vector <span class="math inline">\(v \in V^{*}\)</span>
is said to be a "derivation at <span
class="math inline">\(\boldsymbol{p} \in M^{\prime \prime}\)</span> if
it satisfy the following Leibniz rule (or product rule): <span
class="math display">\[
v(f g)=f(\boldsymbol{p}) v(g)+g(\boldsymbol{p}) v(f) .
\]</span> (Note the similarity with your traditional product rule <span
class="math inline">\((f g)^{\prime}(x)=f(x) g^{\prime}(x)+g(x)
f^{\prime}(x)\)</span>.) Prove the following:</p>
<ol type="1">
<li>Constant functions in <span class="math inline">\(V\)</span> must be
sent to zero by all derivations at any point.</li>
<li>Let <span class="math inline">\(x, y, z \in V\)</span> be the
coordinate function. Suppose <span
class="math inline">\(\boldsymbol{p}=\left[\begin{array}{l}p_{1} \\
p_{2} \\ p_{3}\end{array}\right]\)</span>, then for any derivation <span
class="math inline">\(v\)</span> at <span
class="math inline">\(\boldsymbol{p}\)</span>, then we have <span
class="math inline">\(v\left(\left(x-p_{1}\right)
f\right)=f(\boldsymbol{p}) v(x), v\left(\left(y-p_{2}\right)
f\right)=f(\boldsymbol{p}) v(y)\)</span> and <span
class="math inline">\(v\left(\left(z-p_{3}\right)
f\right)=f(\boldsymbol{p}) v(z)\)</span>.</li>
<li>Let <span class="math inline">\(x, y, z \in V\)</span> be the
coordinate function. Suppose <span
class="math inline">\(\boldsymbol{p}=\left[\begin{array}{l}p_{1} \\
p_{2} \\ p_{3}\end{array}\right]\)</span>, then for any derivation <span
class="math inline">\(v\)</span> at <span
class="math inline">\(\boldsymbol{p}\)</span>, then we have <span
class="math inline">\(v\left(\left(x-p_{1}\right)^{a}\left(y-p_{2}\right)^{b}\left(z-p_{3}\right)^{c}\right)=0\)</span>
for any non-negative integers <span class="math inline">\(a, b,
c\)</span> such that <span
class="math inline">\(a+b+c&gt;1\)</span>.</li>
<li>Let <span class="math inline">\(x, y, z \in V\)</span> be the
coordinate function. Suppose <span
class="math inline">\(\boldsymbol{p}=\left[\begin{array}{l}p_{1} \\
p_{2} \\ p_{3}\end{array}\right]\)</span>, then for any derivation <span
class="math inline">\(v\)</span> at <span
class="math inline">\(\boldsymbol{p}, v(f)=\)</span> <span
class="math inline">\(\dfrac{\partial f}{\partial x}(\boldsymbol{p})
v(x)+\dfrac{\partial f}{\partial y}(\boldsymbol{p}) v(y)+\dfrac{\partial
f}{\partial z}(\boldsymbol{p}) v(z)\)</span>. (Hint: use the Taylor
expansion of <span class="math inline">\(f\)</span> at <span
class="math inline">\(\left.\boldsymbol{p} .\right)\)</span></li>
<li>Any derivation <span class="math inline">\(v\)</span> at <span
class="math inline">\(\boldsymbol{p}\)</span> must be exactly the
directional derivative operator <span
class="math inline">\(\nabla_{\boldsymbol{v}}\)</span> where <span
class="math inline">\(\boldsymbol{v}=\left[\begin{array}{l}v(x) \\ v(y)
\\ v(z)\end{array}\right]\)</span>. (Remark: So, algebraically speaking,
tangent vectors are exactly derivations, i.e., things that satisfy the
Leibniz rule.)</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> Set <span
class="math inline">\(f(\vec{p})\equiv a\)</span>, because <span
class="math inline">\(v(f g)=f(\boldsymbol{p}) v(g)+g(\boldsymbol{p})
v(f)\)</span> and <span class="math inline">\(v\in V^{*}\)</span> is
linear, <span
class="math inline">\(v(fg)=av(g)+g(\boldsymbol{p})v(f)\)</span></p>
<p>Set <span class="math inline">\(g=f\)</span>, so <span
class="math inline">\(v(ff)=2av(f)=v(af)=av(f)\)</span> If <span
class="math inline">\(a=0\)</span>, <span
class="math inline">\(v(g)=v(0)=v(gg)=0+0=0\)</span> If <span
class="math inline">\(a\neq 0\)</span></p>
<p><span class="math inline">\(av(f)=0\)</span>, and <span
class="math inline">\(v(f)=0\)</span> So constant functions in <span
class="math inline">\(V\)</span> are all sent to <span
class="math inline">\(0\)</span></p>
<p><span class="math inline">\((2)\)</span> Calculate <span
class="math inline">\(v((x-p_1)f)=f(\boldsymbol
{p})v(x-p_1)+(x-p_1)(\boldsymbol{p})v(f)\)</span> And <span
class="math inline">\(x(\boldsymbol{p})=p_1,p_1(\boldsymbol{p})=p_1\)</span></p>
<p>So <span class="math inline">\((x-p_1)(\boldsymbol{p})=0\)</span> For
constant <span class="math inline">\(\boldsymbol{p}\)</span>, <span
class="math inline">\(v(\boldsymbol{p}=0)\)</span> So <span
class="math inline">\(v((x-p_1)f)=f(\boldsymbol{p})(v(x)-v(p_1))=f(\boldsymbol{p})v(x)\)</span></p>
<p><span class="math inline">\((3)\)</span> As <span
class="math inline">\(a,b,c\)</span> are all integers, if <span
class="math inline">\(a,b,c&lt;1\)</span>, the sum of them is <span
class="math inline">\(a+b+c\leq 0\)</span>, contradiction</p>
<p>So at least one intergers is <span class="math inline">\(\geq
1\)</span>, without loss of generalization, assume <span
class="math inline">\(a\geq 1\)</span></p>
<p><span
class="math inline">\(v\left(\left(x-p_{1}\right)^{a}\left(y-p_{2}\right)^{b}\left(z-p_{3}\right)^{c}\right)=v(x-p_1)((x-p_1)^{a-1}(y-p_2)^b(z-p_3)^c)(\boldsymbol{p})\)</span></p>
<p>And if <span class="math inline">\(a&gt; 1\)</span>, <span
class="math inline">\((x-p_1)^{a-1}(\boldsymbol{p})=(x-p_1)^{a-2}(x-p_1)(\boldsymbol{p})=0\)</span>
if <span class="math inline">\(a=1\)</span>, then <span
class="math inline">\(b+c&gt;0\)</span></p>
<p>at least one integer in <span class="math inline">\(b\)</span> and
<span class="math inline">\(c\)</span> <span class="math inline">\(\geq
1\)</span>, without of generalization, assume <span
class="math inline">\(b\geq 1\)</span>. Then</p>
<p><span
class="math inline">\((y-p_2)^{b}(\boldsymbol{p})=(y-p_2)^{b-1}(y-p_2)(\boldsymbol{p})=0\)</span>
So <span
class="math inline">\(v\left(\left(x-p_{1}\right)^{a}\left(y-p_{2}\right)^{b}\left(z-p_{3}\right)^{c}\right)=0\)</span></p>
<p><span class="math inline">\((4)\)</span> Use Taylor expansion for
<span class="math inline">\(f\)</span>, we have <span
class="math display">\[
f(x, y, z)=f\left(p_{1}, p_{2}, p_{3}\right)+\frac{\partial f}{\partial
x}(\boldsymbol{p})\left(x-p_{1}\right)+\frac{\partial f}{\partial
y}(\boldsymbol{p})\left(y-p_{2}\right)+\frac{\partial f}{\partial
z}(\boldsymbol{p})\left(z-p_{3}\right)+o(|\boldsymbol{r}-\boldsymbol{p}|)
\]</span> According to <span class="math inline">\((3)\)</span>, the
remainder <span
class="math inline">\(o(|\boldsymbol{r}-\boldsymbol{p}|)=\left(x-p_{1}\right)^{a}\left(y-p_{2}\right)^{b}\left(z-p_{3}\right)^{c},a+b+c&gt;1\)</span></p>
<p>So <span
class="math inline">\(v(o(|\boldsymbol{r}-\boldsymbol{p}|))=0\)</span>
For constant number <span class="math inline">\(v\)</span> sends to
<span class="math inline">\(0\)</span>. Take <span
class="math inline">\(v\)</span> function to the Taylor expansion <span
class="math display">\[
v(f)=\dfrac{\partial f}{\partial x}(\boldsymbol{p}) v(x)+\dfrac{\partial
f}{\partial y}(\boldsymbol{p}) v(y)+\dfrac{\partial f}{\partial
z}(\boldsymbol{p}) v(z)
\]</span> Which shows the complete differential at <span
class="math inline">\(\boldsymbol{p}\)</span></p>
<p><span class="math inline">\((5)\)</span> Just calculate the
directional derivative <span
class="math inline">\(\nabla_{\boldsymbol{v}}\)</span> where <span
class="math inline">\(\boldsymbol{v}=\left[\begin{array}{l}v(x) \\ v(y)
\\ v(z)\end{array}\right]\)</span> <span class="math display">\[
\begin{aligned}
\nabla_{v} f &amp;=\lim _{t \rightarrow 0^{+}} \frac{f(\boldsymbol{p}+t
\boldsymbol{v})-f(\boldsymbol{p})}{t} \\
&amp;=\lim _{t \rightarrow 0^{+}}
\dfrac{f(\boldsymbol{p})+\dfrac{\partial f}{\partial x}(\boldsymbol{p})
v(x) t+\dfrac{\partial f}{\partial y}(\boldsymbol{p}) v(y)
t+\dfrac{\partial f}{\partial z}(\boldsymbol{p}) v(z)
t+o(\|\boldsymbol{v}\| t)-f(\boldsymbol{p})}{t} \\
&amp;=\frac{\partial f}{\partial x}(\boldsymbol{p}) v(x)+\frac{\partial
f}{\partial y}(\boldsymbol{p}) v(y)+\frac{\partial f}{\partial
z}(\boldsymbol{p}) v(z) \\
&amp;=v(f)
\end{aligned}
\]</span> So derivative in calculus is just a vector in the dual space
of <span class="math inline">\(R^{n}\)</span> which suits Leibniz
rule</p>
<h2 id="small-mboxwhat-is-a-vector-field">1.7.3<span
class="math inline">\(\ \small \mbox{What is a vector
field}\)</span></h2>
<p>The discussions in this problem holds for all manifolds <span
class="math inline">\(M\)</span>. But for simplicities sake, suppose
<span class="math inline">\(M=\mathbb{R}^{3}\)</span> for this problem.
Let <span class="math inline">\(V\)</span> be the space of all analytic
functions from <span class="math inline">\(M\)</span> to <span
class="math inline">\(\mathbb{R}\)</span> as usual. We say <span
class="math inline">\(X: V \rightarrow V\)</span> is a vector field on
<span class="math inline">\(X\)</span> if <span
class="math inline">\(X(f g)=f X(g)+g X(f)\)</span>, i.e., the Leibniz
rule again! Prove the following:</p>
<ol type="1">
<li><p>Show that <span class="math inline">\(X_{\boldsymbol{p}}: V
\rightarrow \mathbb{R}\)</span> such that <span
class="math inline">\(X_{\boldsymbol{p}}(f)=(X(f))(\boldsymbol{p})\)</span>
is a derivation at <span class="math inline">\(\boldsymbol{p}\)</span>.
(Hence <span class="math inline">\(X\)</span> is indeed a vector field,
since it is the same as picking a tangent vector at each
point.)</p></li>
<li><p>Note that each <span class="math inline">\(f\)</span> on <span
class="math inline">\(M\)</span> induces a covector field <span
class="math inline">\(\mathrm{d} f\)</span>. Then at each point <span
class="math inline">\(\boldsymbol p\)</span>, the cotangent vector <span
class="math inline">\(\mathrm{d} f\)</span> and the tangent vector <span
class="math inline">\(X\)</span> would evaluate to some number. So <span
class="math inline">\(\mathrm{d} f(X)\)</span> is a function <span
class="math inline">\(M \rightarrow \mathbb{R}\)</span>. Show that <span
class="math inline">\(\mathrm{d} f(X)=X(f)\)</span>, i.e., the two are
the same. (Hint: just use definitions and calculate directly.)</p></li>
<li><p>If <span class="math inline">\(X, Y: V \rightarrow V\)</span> are
vector fields, then note that <span class="math inline">\(X \circ Y: V
\rightarrow V\)</span> might not be a vector field. (Leibniz rule might
fail.) However, show that <span class="math inline">\(X \circ Y-Y \circ
X\)</span> is always a vector field.</p></li>
<li><p>On a related note, show that if <span class="math inline">\(A,
B\)</span> are skew-symmetric matrices, then <span
class="math inline">\(A B-B A\)</span> is still skewsymmetric.
(Skew-symmetric matrices actually corresponds to certain vector fields
on the manifold of orthogonal matrices. So this is no
coincidence.)</p></li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> <span
class="math inline">\(X_{\boldsymbol{p}}(fg)=(X(fg))(\boldsymbol{p})=(fX(g)+gX(f))(\boldsymbol{p})=f(\boldsymbol{p})X(g)(\boldsymbol{p})+g(\boldsymbol{p})X(f)(\boldsymbol{p})\)</span></p>
<p><span
class="math inline">\(=f(\boldsymbol{p})X_{\boldsymbol{p}}(g)+g(\boldsymbol{p})X_{\boldsymbol{p}}(f)\)</span>
So it suits the definition of derivative at <span
class="math inline">\(\boldsymbol{p}\)</span></p>
<p><span class="math inline">\((2)\)</span> <span
class="math inline">\(X_{\boldsymbol{p}}:V\rightarrow R\)</span> So
<span class="math inline">\(X_{\boldsymbol{p}}\in V^{*}\)</span> <span
class="math inline">\(X_{p}=\left[\begin{array}{c} X_{\boldsymbol{p}}(X)
&amp; X_{p}(Y) &amp; X_{p}(Z) \end{array}\right]\)</span></p>
<p><span class="math inline">\(df(X)(\boldsymbol{p})=df_{\boldsymbol{p}
}(X_{\boldsymbol{p}})\)</span> where <span
class="math inline">\(df=\left[\begin{array}{c} \dfrac{\partial
f}{\partial x}(\boldsymbol{p})\\ \dfrac{\partial f}{\partial
y}(\boldsymbol{p})\\ \dfrac{\partial f}{\partial z}(\boldsymbol{p})\\
\end{array}\right]\)</span> combine them together then we have</p>
<p>From <span class="math inline">\(\small (1.4)\)</span> <span
class="math inline">\((X(f))(\boldsymbol{p})=X_{p}(f)=\dfrac{\partial
f}{\partial x}(\boldsymbol{p}) X_{\boldsymbol{p}}(X)+\dfrac{\partial
f}{\partial y}(\boldsymbol{p}) X_{\boldsymbol{p}}(Y)+\dfrac{\partial
f}{\partial z}(\boldsymbol{p}) X_{\boldsymbol{p}}(Z)\)</span></p>
<p><span class="math inline">\(=\left[\begin{array}{c}
X_{\boldsymbol{p}}(X) &amp; X_{p}(Y) &amp; X_{p}(Z)
\end{array}\right]\left[\begin{array}{c} \dfrac{\partial f}{\partial
x}(\boldsymbol{p})\\ \dfrac{\partial f}{\partial y}(\boldsymbol{p})\\
\dfrac{\partial f}{\partial z}(\boldsymbol{p})\\
\end{array}\right]=df(X)\)</span></p>
<p><span class="math inline">\((3)\)</span> Just unfold the fomula <span
class="math display">\[
\begin{gathered}
(X\circ Y-Y\circ X)(fg)=X\circ Y(fg)-Y\circ X(fg)=X\circ
(fY(g)+gY(f))-Y\circ (fX(g)+gX(f))\\
=\small f (X\circ Y)g+X(f)Y(g)+g (X\circ Y)f+X(g)Y(f)-(f (Y\circ
X)g+Y(f)X(g)+g (Y\circ X)f+Y(g)X(f))\\
=f(X\circ Y-Y\circ X)g+g(X\circ Y-Y\circ X)f
\end{gathered}
\]</span> So <span class="math inline">\(X\circ Y-Y \circ X\)</span>
suits the Leibniz rule, which is a vector field.</p>
<p><span class="math inline">\((4)\)</span> Calculate directly we can
prove <span class="math inline">\((AB-BA)^{T}=-(AB-BA)\)</span> <span
class="math display">\[
\begin{aligned}
(A B-B A)^{T} =(A B)^{T}-(B A)^{T}
=(B^{T} A^{T}-A^{T} B^{T}) \\
=(-B)(-A)-(-A)(-B) =BA-AB=-(A B-B A)
\end{aligned}
\]</span> For all the positive orthogonal matrices <span
class="math inline">\(\mathcal{Q}\)</span> <span
class="math inline">\(\forall \ Q\in \mathcal{Q},\det(Q)=1\)</span>. At
one matrix, its tangent vector</p>
<p><span
class="math inline">\((Q+A)^{T}(Q+A)=I=(Q^{T}+A^{T})(Q+A)=I\Longrightarrow
A=-A^{T},||A||\to 0\)</span></p>
<p>are all Skew-symmetric matrices. According to <span
class="math inline">\(\small (3)\)</span>, <span
class="math inline">\(AB-BA\)</span> is also a Skew-symmetric matrix</p>
<blockquote>
<p><span class="math inline">\(\Large \mathbf{If\ your\ life\ is\
tense,\ it\ could\ be\ a\ tensor. }\)</span></p>
</blockquote>
<h1
id="largetextcolorbluemboxadvanced-algebra-small-mathbbhwmathrm8-_textcolorblue2022.5.9"><span
class="math inline">\(\large\textcolor{blue}{\mbox{Advanced Algebra }
\small \mathbb{HW}\mathrm{8}}\ \ \ \ \ \
_\textcolor{blue}{2022.5.9}\)</span></h1>
<h2 id="small-mboxelementary-layer-operations-for-tensors">1.8.1<span
class="math inline">\(\ \small \mbox{Elementary layer operations for
tensors}\)</span></h2>
<p>Note that, for "2D" matrices we have row and column operations, and
the two kinds of operations corresponds to the two dimensions of the
array.</p>
<p>For simplicity, let <span class="math inline">\(M\)</span> be a <span
class="math inline">\(2 \times 2 \times 2\)</span> "3D matrix". Then we
have "row layer operations", "column layer operations", "horizontal
layer operations". The three kinds corresponds to the three dimensions
of the array. We interpret this as a multilinear map <span
class="math inline">\(M: \mathbb{R}^{2} \times \mathbb{R}^{2} \times
\mathbb{R}^{2} \rightarrow \mathbb{R}\)</span>. Let <span
class="math inline">\(\left(\left(\mathbb{R}^{2}\right)^{*}\right)^{\otimes
3}\)</span> be the space of all multilinear maps from <span
class="math inline">\(\mathbb{R}^{2} \times \mathbb{R}^{2} \times
\mathbb{R}^{2}\)</span> to <span
class="math inline">\(\mathbb{R}\)</span>.</p>
<ol type="1">
<li>Given <span class="math inline">\(\alpha, \beta, \gamma
\in\left(\mathbb{R}^{2}\right)^{*}\)</span>, what is the <span
class="math inline">\((i, j, k)\)</span>-entry of the "3D matrix" <span
class="math inline">\(\alpha \otimes \beta \otimes \gamma\)</span> in
terms of the coordinates of <span class="math inline">\(\alpha, \beta,
\gamma\)</span> ? Here <span class="math inline">\(\alpha \otimes \beta
\otimes \gamma\)</span> is the multilinear map sending <span
class="math inline">\((\boldsymbol{u}, \boldsymbol{v},
\boldsymbol{w})\)</span> to the real number <span
class="math inline">\(\alpha(\boldsymbol{u}) \beta(\boldsymbol{v})
\gamma(\boldsymbol{w})\)</span>.</li>
<li>Let <span class="math inline">\(E\)</span> be an elementary matrix.
Then we can send <span class="math inline">\(\alpha \otimes \beta
\otimes \gamma\)</span> to <span class="math inline">\((\alpha E)
\otimes \beta \otimes \gamma\)</span>. Why can this be extended to a
linear map <span
class="math inline">\(M_{E}:\left(\left(\mathbb{R}^{2}\right)^{*}\right)^{\otimes
3} \rightarrow\left(\left(\mathbb{R}^{2}\right)^{*}\right)^{\otimes
3}\)</span> ? (This gives a formula for the "elementary layer
operations" on "3D matrices", where the three kinds of layer operations
corresponds to applying <span class="math inline">\(E\)</span> to the
three arguments respectively.)</li>
<li>Show that elementary layer operations preserve rank. Here we say
<span class="math inline">\(M\)</span> has rank <span
class="math inline">\(r\)</span> if <span
class="math inline">\(r\)</span> is the smallest possible integer such
that <span class="math inline">\(M\)</span> can be written as the linear
combination of <span class="math inline">\(r\)</span> "rank one" maps,
i.e., maps of the kind <span class="math inline">\(\alpha \otimes \beta
\otimes \gamma\)</span> for some <span class="math inline">\(\alpha,
\beta, \gamma \in\left(\mathbb{R}^{2}\right)^{*}\)</span>.</li>
<li>Show that, if some "2D" layer matrix of a "3D matrix" has rank r,
then the <span class="math inline">\(3 D\)</span> matrix has rank at
least <span class="math inline">\(r\)</span>.</li>
<li>Let <span class="math inline">\(M\)</span> be made of two layers,
<span class="math inline">\(\left[\begin{array}{ll}1 &amp; 0 \\ 0 &amp;
1\end{array}\right]\)</span> and <span
class="math inline">\(\left[\begin{array}{ll}0 &amp; 1 \\ 1 &amp;
0\end{array}\right]\)</span>. Find its rank.</li>
<li>(Read only) Despite some practical interests, finding the tensor
rank in general is NOT easy. In fact, it is NP-complete just for
3-tensors over finite field. Furthermore, a tensor with all real entries
might have different real rank and complex rank.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> According to the symmetry of
dot product <span class="math inline">\(\langle\alpha,u\rangle=\langle
u,\alpha\rangle\)</span> we have equation <span
class="math inline">\(\alpha^Tu=u^T\alpha\)</span> So</p>
<p><span class="math inline">\(\alpha^{T} u \beta^{T} v \gamma^{T}
\omega=u^{T} \alpha \beta^{T} v \gamma^{T} \omega=u^{T}\left[\alpha
\beta^{T} v \gamma_{1} \quad \partial \beta^{T} v \gamma_{2}\right]
\omega\)</span> Compare to <span class="math inline">\([u^TA_1v\quad
u^TA_2v]\omega\)</span> hence</p>
<p><span
class="math inline">\(A_{1}=\gamma_{1}\left(\begin{array}{ll}\alpha_{1}
\beta_{1} &amp; \alpha_{1} \beta_{2} \\ \alpha_{2} \beta_{1} &amp;
\alpha_{2}
\beta_{2}\end{array}\right)=\gamma_1\alpha\beta^T,A_{2}=\gamma_{2}\left(\begin{array}{ll}\alpha_{1}
\beta_{1} &amp; \alpha_{1} \beta_{2} \\ \alpha_{2} \beta_{1} &amp;
\alpha_{2} \beta_{2}\end{array}\right)=\gamma_2\alpha\beta^T\)</span> .
So <span
class="math inline">\(A_{ijk}=\gamma_i\alpha_j\beta_k\)</span></p>
<p><span class="math inline">\((2)\)</span> <span
class="math inline">\(M_E\)</span> sends tensor <span
class="math inline">\(M=[[A_1,A_2]]=[[\gamma_1\alpha
\beta^T,\gamma_2\alpha \beta^T]]\)</span> to <span
class="math inline">\(M&#39;=[[\gamma_1\alpha E \beta^T,\gamma_2\alpha
E\beta^T]]\)</span></p>
<p>For <span class="math inline">\(\alpha _1,\alpha_2\)</span> in <span
class="math inline">\((\mathbb{R}^2)^*\)</span> as one part in <span
class="math inline">\(((\mathbb{R}^2)^*)^{\otimes3}\)</span> <span
class="math display">\[
M_{k\alpha_1+\mu\alpha_2}=[[\gamma_1(k\alpha_1+\mu\alpha_2)
\beta^T,\gamma_2(k\alpha_1+\mu\alpha_2) \beta^T]]=kM_{\alpha _1}+\mu
M_{\alpha _2}\in ((\mathbb{R}^2)^*)^{\otimes3}
\]</span> So for <span class="math inline">\(\alpha ,\beta\)</span> is
linear. and for <span class="math inline">\(\gamma_1,\gamma_2\)</span>
in <span class="math inline">\((\mathbb{R}^2)^*\)</span> as one part in
<span class="math inline">\(((\mathbb{R}^2)^*)^{\otimes3}\)</span>, set
<span class="math inline">\(\gamma_{11,12}\)</span> as the component of
<span class="math inline">\(\gamma_1\)</span></p>
<p><span class="math inline">\(\gamma_{21,22}\)</span> as the component
of <span class="math inline">\(\gamma_2\)</span>, So <span
class="math display">\[
M_{k\gamma_1+\mu\gamma_2}=[[(k\gamma_{11}+\mu\gamma_{21})\alpha\beta^T,(k\gamma_{12}+\mu\gamma_{22})\alpha
\beta^T]]=kM_{\gamma _1}+\mu M_{\gamma _2}\in
((\mathbb{R}^2)^*)^{\otimes3}
\]</span> So <span class="math inline">\(M_E\)</span> is a linear map,
three operations at <span
class="math inline">\(\alpha,\beta,\gamma\)</span></p>
<p><span class="math inline">\((3)\)</span> Suppose <span
class="math inline">\(M=\displaystyle \sum_{i=1}^rM_{base(i)}\)</span>
if we operate elementary layer operations for <span
class="math inline">\(M\)</span>, the right hand side</p>
<p>is also in "rank one" maps, so <span class="math inline">\(r&#39;\leq
r\)</span>. And if <span class="math inline">\(r&#39;&lt;r\)</span>,
i.e., <span class="math inline">\(M&#39;=\displaystyle
\sum_{i=1}^{r&#39;}M_{base(i)}\)</span> As elementary have its
inverse</p>
<p>operate the inverse of elementary operation, and we have
contradiction, so <span class="math inline">\(r&#39;=r\)</span></p>
<p><span class="math inline">\((4)\)</span> According to <span
class="math inline">\(\mbox{SVD}\)</span>, the minimum number of
decomposing a matrix into rank-<span class="math inline">\(1\)</span>
matrixes equals to rank</p>
<p>So if <span class="math inline">\(2D\)</span> matrix needs at least
<span class="math inline">\(r\)</span> rank-<span
class="math inline">\(1\)</span> matrixes to make up, since every
rank-<span class="math inline">\(1\)</span> maps in <span
class="math inline">\(\alpha \otimes \beta \otimes \gamma\)</span></p>
<p>contains two rank-<span class="math inline">\(1\)</span> matrix, so
the <span class="math inline">\(3D\)</span> matrix also needs at least
<span class="math inline">\(r\)</span> rank-<span
class="math inline">\(1\)</span> tensors to make up</p>
<p><span class="math inline">\((5)\)</span> For <span
class="math inline">\(A_1=\left[\begin{array}{ll}1 &amp; 0 \\ 0 &amp;
1\end{array}\right]\)</span> its rank is <span
class="math inline">\(2\)</span>, so <span
class="math inline">\(r(M)\geq 2\)</span>. Besides, construct two
rank-<span class="math inline">\(1\)</span> tensors <span
class="math display">\[
M_1=[[\dfrac{1}{2}\begin{bmatrix}1 &amp; 1 \\ 1 &amp;
1\end{bmatrix},\dfrac{1}{2}\begin{bmatrix}1 &amp; 1 \\ 1 &amp;
1\end{bmatrix}]],
M_2=[[\dfrac{1}{2}\begin{bmatrix}1 &amp; -1 \\ -1 &amp;
1\end{bmatrix},-\dfrac{1}{2}\begin{bmatrix}1 &amp; -1 \\ -1 &amp;
1\end{bmatrix}]]
\]</span> And <span class="math inline">\(M=M_1+M_2\)</span>, so its
rank is <span class="math inline">\(2\)</span></p>
<p><img
src="https://pic.imgdb.cn/item/6278cb2909475431290238c1.jpg" srcset="/img/loading.gif" lazyload /></p>
<h2 id="small-mboxijk-rank-3-tensor">1.8.2<span class="math inline">\(\
\small \mbox{i+j+k rank-3 tensor}\)</span></h2>
<p>Let <span class="math inline">\(M\)</span> be a <span
class="math inline">\(3 \times 3 \times 3\)</span> "3D matrix" whose
<span class="math inline">\((i, j, k)\)</span>-entry is <span
class="math inline">\(i+j+k\)</span>. We interpret this as a multilinear
map <span class="math inline">\(M: \mathbb{R}^{3} \times \mathbb{R}^{3}
\times \mathbb{R}^{3} \rightarrow \mathbb{R}\)</span>.</p>
<ol type="1">
<li>Let <span
class="math inline">\(\boldsymbol{v}=\left[\begin{array}{l}x \\ y \\
z\end{array}\right]\)</span>, then <span
class="math inline">\(M(\boldsymbol{v}, \boldsymbol{v},
\boldsymbol{v})\)</span> is a polynomial in <span
class="math inline">\(x, y, z\)</span>. What is this polynomial?</li>
<li>Let <span class="math inline">\(\sigma:\{1,2,3\}
\rightarrow\{1,2,3\}\)</span> be any bijection. Show that <span
class="math inline">\(M\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2},
\boldsymbol{v}_{3}\right)=M\left(\boldsymbol{v}_{\sigma(1)},
\boldsymbol{v}_{\sigma(2)}, \boldsymbol{v}_{\sigma(3)}\right)\)</span>.
(Hint: brute force works. But alternatively, try find the <span
class="math inline">\((i, j, k)\)</span> entry of the multilinear map
<span class="math inline">\(M^{\sigma}\)</span>, a map that sends <span
class="math inline">\(\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2},
\boldsymbol{v}_{3}\right)\)</span> to <span
class="math inline">\(M\left(\boldsymbol{v}_{\sigma(1)},
\boldsymbol{v}_{\sigma(2)},
\boldsymbol{v}_{\sigma(3)}\right)\)</span>.)</li>
<li>Show that the rank <span class="math inline">\(r\)</span> of <span
class="math inline">\(M\)</span> is at least 2 and at most 3. (It is
actually exactly three.)</li>
<li>(Read only) Any study of polynomial of degree <span
class="math inline">\(d\)</span> on <span
class="math inline">\(n\)</span> variables is equivalent to the study of
some symmetric <span class="math inline">\(d\)</span> tensor on <span
class="math inline">\(\mathbb{R}^{n}\)</span>.</li>
</ol>
<hr />
<p><span class="math inline">\((1)\)</span> <span
class="math inline">\(M=[[\begin{bmatrix}3 &amp; 4 &amp; 5 \\ 4 &amp; 5
&amp; 6\\5&amp; 6&amp;7\end{bmatrix},\begin{bmatrix}4 &amp; 5 &amp; 6 \\
5 &amp; 6 &amp; 7\\6&amp; 7&amp;8\end{bmatrix},\begin{bmatrix}5 &amp; 6
&amp; 7 \\ 6 &amp; 7 &amp; 8\\7&amp;
8&amp;9\end{bmatrix}]]=[[A_1,A_2,A_3]]\)</span> And <span
class="math inline">\(M(\boldsymbol v,\boldsymbol v,\boldsymbol
v)\)</span> where <span
class="math inline">\(\boldsymbol{v}=\begin{pmatrix}x\\y\\z\end{pmatrix}\)</span></p>
<p><span class="math inline">\(=[\boldsymbol v^TA_1\boldsymbol v\quad
\boldsymbol v^TA_2\boldsymbol v\quad \boldsymbol v^TA_3\boldsymbol
v]\boldsymbol v=[\boldsymbol v^T\begin{bmatrix}3 &amp; 4 &amp; 5 \\ 4
&amp; 5 &amp; 6\\5&amp; 6&amp;7\end{bmatrix}\boldsymbol v\quad
\boldsymbol v^T\begin{bmatrix}4 &amp; 5 &amp; 6 \\ 5 &amp; 6 &amp;
7\\6&amp; 7&amp;8\end{bmatrix}\boldsymbol v\quad \boldsymbol
v^T\begin{bmatrix}5 &amp; 6 &amp; 7 \\ 6 &amp; 7 &amp; 8\\7&amp;
8&amp;9\end{bmatrix}\boldsymbol v]\boldsymbol v\)</span></p>
<p>Calculate it by <span
class="math inline">\(\mbox{Mathematica}\)</span>, the result is <span
class="math inline">\(p(x,y,z)=3 (x+y+z)^2 (x+2 y+3 z)\)</span></p>
<p><img
src="https://pic.imgdb.cn/item/6278d61109475431291e44d0.jpg" srcset="/img/loading.gif" lazyload /></p>
<p><span class="math inline">\((2)\)</span> let a linear map from <span
class="math inline">\(\mathbb{R}^{3} \times \mathbb{R}^{3} \times
\mathbb{R}^{3}\)</span> to <span
class="math inline">\(\mathbb{R}\)</span> sends <span
class="math inline">\(\left(\boldsymbol{v}_{1}, \boldsymbol{v}_{2},
\boldsymbol{v}_{3}\right)\)</span> to <span
class="math inline">\(M\left(\boldsymbol{v}_{\sigma(1)},
\boldsymbol{v}_{\sigma(2)},
\boldsymbol{v}_{\sigma(3)}\right)\)</span></p>
<p>Obviously, it's multi-linear since for <span
class="math inline">\(\boldsymbol{v}_i\)</span> the evaluation result is
linear no matter which position <span
class="math inline">\(\boldsymbol{v}_i\)</span> is.</p>
<p>And this map have a tensor such <span
class="math inline">\(M&#39;=[[A_1&#39;,A_2&#39;,A_3&#39;]]\)</span>.
Specialise <span class="math inline">\(\boldsymbol{v_i}\)</span> to get
value of <span
class="math inline">\(A_{1}&#39;,A_{2}&#39;,A_{3}&#39;\)</span></p>
<p>Set <span
class="math inline">\(\boldsymbol{b}_1=\begin{pmatrix}1\\0\\0\end{pmatrix},\boldsymbol{b}_2=\begin{pmatrix}0\\1\\0\end{pmatrix},\boldsymbol{b}_3=\begin{pmatrix}0\\0\\1\end{pmatrix}\)</span>
let <span
class="math inline">\((\boldsymbol{v_1},\boldsymbol{v_2},\boldsymbol{v_3})=(\boldsymbol{b_i},\boldsymbol{b_j},\boldsymbol{b_k})\)</span>
where <span class="math inline">\(1\leq i,j,k\leq 3\)</span></p>
<p>and they can be the same. Put one of condition into the map so <span
class="math display">\[
A_{k(ij)}&#39;=A_{\sigma^{-1}(i)(\sigma^{-1}(j)\sigma^{-1}(k))}=\sigma^{-1}(i)+\sigma^{-1}(j)+\sigma^{-1}(k)=i+j+k
\]</span> So for <span class="math inline">\(k=1,2,3\)</span> <span
class="math inline">\(A_{k}&#39;=A_k\)</span>, which implies that <span
class="math inline">\(M&#39;=M\)</span>, so the equation is proved.</p>
<blockquote>
<p><strong>A brute try</strong> (failed):</p>
<p>It is obvious that swaping <span
class="math inline">\(\boldsymbol{v}_{i},\boldsymbol{v}_{j}\)</span> at
most twice can make <span
class="math inline">\(\boldsymbol{v}_1,\boldsymbol{v}_2,\boldsymbol{v}_3\)</span>
$$ <span class="math inline">\(\boldsymbol{v}_{\sigma(1)},
\boldsymbol{v}_{\sigma(2)}, \boldsymbol{v}_{\sigma(3)}\)</span></p>
<p>If swap <span
class="math inline">\(\boldsymbol{v}_{1},\boldsymbol{v}_{2}\)</span>,
the value <span class="math inline">\(\boldsymbol v_1^TA_i\boldsymbol
v_2=\boldsymbol v_2^TA_i\boldsymbol v_1,i=1,2,3\)</span>, since <span
class="math inline">\(A_i=A_i^T\)</span> so the result stays the
same.</p>
<p>And if we swap <span
class="math inline">\(\boldsymbol{v}_{2},\boldsymbol{v}_{3}\)</span>,
<span class="math inline">\([\boldsymbol v_1^TA_1\boldsymbol v_2\quad
\boldsymbol v_1^TA_2\boldsymbol v_2\quad \boldsymbol v_1^TA_3\boldsymbol
v_2]\boldsymbol v_3=\boldsymbol v_1^TA_1\boldsymbol
v_2v_{3x}+\boldsymbol v_1^TA_2\boldsymbol v_2v_{3y}+\boldsymbol
v_1^TA_3\boldsymbol v_2v_{3z}\)</span></p>
</blockquote>
<p><span class="math inline">\((3)\)</span> According to <span
class="math inline">\(1.8.1(4)\)</span>, since <span
class="math inline">\(\mbox{rank}(A_i)=2\)</span>, so <span
class="math inline">\(r\geq 2\)</span>. Then just construct a reasonable
combination</p>
<p>I guess <span
class="math inline">\(\alpha=\begin{pmatrix}1\\1\\-1\end{pmatrix},\beta=\begin{pmatrix}1\\-1\\1\end{pmatrix},\gamma=\begin{pmatrix}-1\\1\\1\end{pmatrix}\)</span>
so the rank-<span class="math inline">\(1\)</span> matrix like <span
class="math display">\[
\begin{bmatrix}a&amp;b&amp;c\\a&amp;b&amp;c\\-a&amp;-b&amp;-c\end{bmatrix},\begin{bmatrix}d&amp;e&amp;f\\-d&amp;-e&amp;-f\\d&amp;e&amp;f\end{bmatrix},\begin{bmatrix}-g&amp;-h&amp;-i\\g&amp;h&amp;i\\g&amp;h&amp;i\end{bmatrix},
\]</span> And the linear combinations of these three matrixes are <span
class="math inline">\(\begin{bmatrix}3 &amp; 4 &amp; 5 \\ 4 &amp; 5
&amp; 6\\5&amp; 6&amp;7\end{bmatrix},\begin{bmatrix}4 &amp; 5 &amp; 6 \\
5 &amp; 6 &amp; 7\\6&amp; 7&amp;8\end{bmatrix},\begin{bmatrix}5 &amp; 6
&amp; 7 \\ 6 &amp; 7 &amp; 8\\7&amp; 8&amp;9\end{bmatrix}\)</span></p>
<p>which transfers to three nine - dimensional equations, out of my hand
ability, the coefficient matrix is <span class="math display">\[
A=\left(
\begin{array}{ccccccccc}
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 \\
  0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
\end{array}
\right),A\vec{x}=\vec{b}_{i}=\begin{pmatrix}3\\4\\5\\4\\5\\6\\5\\6\\7\end{pmatrix},\begin{pmatrix}4\\5\\6\\5\\6\\7\\6\\7\\8\end{pmatrix},\begin{pmatrix}5\\6\\7\\6\\7\\8\\7\\8\\9\end{pmatrix}
\]</span> And the accurate solution is <span class="math display">\[
\begin{gathered}
\begin{bmatrix}3 &amp; 4 &amp; 5 \\ 4 &amp; 5 &amp; 6\\5&amp;
6&amp;7\end{bmatrix}=\begin{bmatrix}3.5&amp;4.5&amp;5.5\\3.5&amp;4.5&amp;5.5\\-3.5&amp;-4.5&amp;-5.5\end{bmatrix}+
\begin{bmatrix}4&amp;5&amp;6\\-4&amp;-5&amp;-6\\4&amp;5&amp;6\end{bmatrix}+\begin{bmatrix}-4.5&amp;-5.5&amp;-6.5\\4.5&amp;5.5&amp;6.5\\4.5&amp;5.5&amp;6.5\end{bmatrix}\\
\begin{bmatrix}4 &amp; 5 &amp; 6 \\ 5 &amp; 6 &amp; 7\\6&amp;
7&amp;8\end{bmatrix}=\begin{bmatrix}4.5&amp;5.5&amp;6.5\\4.5&amp;5.5&amp;6.5\\-4.5&amp;-5.5&amp;-6.5\end{bmatrix}+
\begin{bmatrix}5&amp;6&amp;7\\-5&amp;-6&amp;-7\\5&amp;6&amp;7\end{bmatrix}+\begin{bmatrix}-5.5&amp;-6.5&amp;-7.5\\5.5&amp;6.5&amp;7.5\\5.5&amp;6.5&amp;7.5\end{bmatrix}\\
\begin{bmatrix}5 &amp; 6 &amp; 7 \\ 6 &amp; 7 &amp; 8\\7&amp;
8&amp;9\end{bmatrix}=\begin{bmatrix}5.5&amp;6.5&amp;7.5\\5.5&amp;6.5&amp;7.5\\-5.5&amp;-6.5&amp;-7.5\end{bmatrix}+
\begin{bmatrix}6&amp;7&amp;8\\-6&amp;-7&amp;-8\\6&amp;7&amp;8\end{bmatrix}+\begin{bmatrix}-6.5&amp;-7.5&amp;-8.5\\6.5&amp;7.5&amp;8.5\\6.5&amp;7.5&amp;8.5\end{bmatrix}\\
\end{gathered}
\]</span> So it can be decomposed into three rank-<span
class="math inline">\(1\)</span> tensors. So <span
class="math inline">\(r\leq 3\)</span></p>

              
            </div>
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%95%B0%E5%AD%A6/" class="category-chain-item">数学</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%95%B0%E5%AD%A6/%E9%AB%98%E7%AD%89%E4%BB%A3%E6%95%B0%E9%80%89%E8%AE%B2/" class="category-chain-item">高等代数选讲</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Advanced Algebra part of Homework</div>
      <div>https://lr-tsinghua11.github.io/2022/03/16/%E6%95%B0%E5%AD%A6/%E9%AB%98%E4%BB%A3%E9%80%89%E8%AE%B2%E9%83%A8%E5%88%86%E4%BD%9C%E4%B8%9A/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Learning_rate</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年3月16日</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>许可协议</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - 署名">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/03/18/%E7%BC%96%E7%A8%8B/gdb%E8%B0%83%E8%AF%95%E5%99%A8/" title="gdb to debug">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">gdb to debug</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/03/03/%E7%BC%96%E7%A8%8B/python%20%E5%8F%AF%E8%A7%86%E5%8C%96%E6%B5%81%E7%A8%8B/" title="Python-Visualization-Process the Json file">
                        <span class="hidden-mobile">Python-Visualization-Process the Json file</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.13.10/mermaid.min.js', function() {
    mermaid.initialize({"theme":"forest"});
  });
</script>





    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  
    <script  src="/js/img-lazyload.js" ></script>
  



  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var title = subtitle.title;
      
        typing(title);
      
    })(window, document);
  </script>




  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?2d5b78dfbf046ab610d306e42da0ed37";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  

  
    
  





  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.0/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script>
  (function() {
    var enableLang = CONFIG.code_language.enable && CONFIG.code_language.default;
    var enableCopy = CONFIG.copy_btn;
    if (!enableLang && !enableCopy) {
      return;
    }

    function getBgClass(ele) {
      return Fluid.utils.getBackgroundLightness(ele) >= 0 ? 'code-widget-light' : 'code-widget-dark';
    }

    var copyTmpl = '';
    copyTmpl += '<div class="code-widget">';
    copyTmpl += 'LANG';
    copyTmpl += '</div>';
    jQuery('.markdown-body pre').each(function() {
      var $pre = jQuery(this);
      if ($pre.find('code.mermaid').length > 0) {
        return;
      }
      if ($pre.find('span.line').length > 0) {
        return;
      }

      var lang = '';

      if (enableLang) {
        lang = CONFIG.code_language.default;
        if ($pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2 && $pre.children().hasClass('hljs')) {
          lang = $pre[0].children[0].classList[1];
        } else if ($pre[0].getAttribute('data-language')) {
          lang = $pre[0].getAttribute('data-language');
        } else if ($pre.parent().hasClass('sourceCode') && $pre[0].children.length > 0 && $pre[0].children[0].classList.length >= 2) {
          lang = $pre[0].children[0].classList[1];
          $pre.parent().addClass('code-wrapper');
        } else if ($pre.parent().hasClass('markdown-body') && $pre[0].classList.length === 0) {
          $pre.wrap('<div class="code-wrapper"></div>');
        }
        lang = lang.toUpperCase().replace('NONE', CONFIG.code_language.default);
      }
      $pre.append(copyTmpl.replace('LANG', lang).replace('code-widget">',
        getBgClass($pre[0]) + (enableCopy ? ' code-widget copy-btn" data-clipboard-snippet><i class="iconfont icon-copy"></i>' : ' code-widget">')));

      if (enableCopy) {
        Fluid.utils.createScript('https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js', function() {
          var clipboard = new window.ClipboardJS('.copy-btn', {
            target: function(trigger) {
              var nodes = trigger.parentNode.childNodes;
              for (var i = 0; i < nodes.length; i++) {
                if (nodes[i].tagName === 'CODE') {
                  return nodes[i];
                }
              }
            }
          });
          clipboard.on('success', function(e) {
            e.clearSelection();
            e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-copy', 'icon-success');
            setTimeout(function() {
              e.trigger.innerHTML = e.trigger.innerHTML.replace('icon-success', 'icon-copy');
            }, 2000);
          });
        });
      }
    });
  })();
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        MathJax = {
          tex    : {
            inlineMath: { '[+]': [['$', '$']] }
          },
          loader : {
            load: ['ui/lazy']
          },
          options: {
            renderActions: {
              findScript    : [10, doc => {
                document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                  const display = !!node.type.match(/; *mode=display/);
                  const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                  const text = document.createTextNode('');
                  node.parentNode.replaceChild(text, node);
                  math.start = { node: text, delim: '', n: 0 };
                  math.end = { node: text, delim: '', n: 0 };
                  doc.math.push(math);
                });
              }, '', false],
              insertedScript: [200, () => {
                document.querySelectorAll('mjx-container').forEach(node => {
                  let target = node.parentNode;
                  if (target.nodeName.toLowerCase() === 'li') {
                    target.parentNode.classList.add('has-jax');
                  }
                });
              }, '', false]
            }
          }
        };
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</body>
</html>
